import requests
import ddddocr
import random
import csv
import time
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

class CrazyCarScraper:
    def __init__(self, username, password, log_callback=None):
        self.base_url = "https://ckfksc.com"
        self.username = username
        self.password = password
        self.log_callback = log_callback
        
        self.session = requests.Session()
        # å¢åŠ è¿æ¥æ± å¤§å°ï¼Œé€‚åº”å¤šçº¿ç¨‹
        adapter = requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=20)
        self.session.mount('https://', adapter)
        self.session.mount('http://', adapter)
        
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
            "X-Requested-With": "XMLHttpRequest",
            "Referer": f"{self.base_url}/login",
        })
        self.ocr = ddddocr.DdddOcr(show_ad=False)
        self.is_running = False

    def log(self, message):
        if self.log_callback:
            self.log_callback(message)
        else:
            print(message)

    def login(self):
        self.log(f"ğŸš€ æ­£åœ¨ç™»å½•è´¦å·: {self.username} ...")
        try:
            self.session.get(f"{self.base_url}/login")
            captcha_resp = self.session.get(
                f"{self.base_url}/captcha?r={random.random()}")
            
            captcha_code = self.ocr.classification(
                captcha_resp.content).strip().upper()
            
            payload = {
                "areaCode": "86", 
                "mobileNo": self.username,
                "password": self.password, 
                "captcha": captcha_code
            }
            
            res = self.session.post(
                f"{self.base_url}/login", data=payload).json()
            
            if res.get("respCo") == "0000":
                self.log(f"âœ… ç™»å½•æˆåŠŸ! éªŒè¯ç : [{captcha_code}]")
                return True
            
            self.log(f"âŒ ç™»å½•å¤±è´¥: {res.get('respMsg')}")
            return False
        except Exception as e:
            self.log(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
            return False

    def clean_data(self, records, substitution_map):
        if not substitution_map:
            return records
        cleaned = []
        for r in records:
            new_row = []
            for item in r:
                s_item = str(item).replace('\xa0', ' ').strip()
                new_row.append(substitution_map.get(s_item, s_item))
            cleaned.append(new_row)
        return cleaned

    def fetch_detail(self, summary_data, detail_url):
        """å•ç‹¬æŠ“å–ä¸€ä¸ªè¯¦æƒ…é¡µçš„çº¿ç¨‹å‡½æ•°"""
        try:
            resp = self.session.get(detail_url, timeout=10)
            d_soup = BeautifulSoup(resp.text, "html.parser")
            
            # æå–è¡¨å¤´ (å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡)
            header = None
            h_row = d_soup.select_one("table.table thead tr")
            if h_row:
                header = ["æ¨¡å¼", "åœ°å›¾", "å¼€å§‹æ—¶é—´"] + [th.text.strip() for th in h_row.find_all("th")]

            # æå–æ•°æ®è¡Œ
            rows = []
            for dr in d_soup.select("table.table tbody tr"):
                row_data = summary_data + [td.text.strip() for td in dr.find_all("td")]
                rows.append(row_data)
                
            return header, rows
        except Exception as e:
            # self.log(f"âš ï¸ è¯¦æƒ…æŠ“å–å¤±è´¥: {e}") # çº¿ç¨‹ä¸­å°½é‡å°‘logï¼Œé¿å…UIå¡é¡¿
            return None, []

    def start_crawl(self, game_type, start_maps, end_map, substitution_map=None):
        self.is_running = True
        
        game_modes = ["ä¸ªäººç«é€Ÿ", "ç»„é˜Ÿç«é€Ÿ", "ä¸ªäººé“å…·", "ç»„é˜Ÿé“å…·", "ä¸ªäººç–¾çˆ½", "ç»„é˜Ÿç–¾çˆ½"]
        if game_type in game_modes:
            game_type_id = str(game_modes.index(game_type))
        else:
            game_type_id = game_type

        if isinstance(start_maps, str):
            start_maps = [start_maps]

        self.log(f"\nğŸ•·ï¸ [æŠ“å–å¼€å§‹] æ¨¡å¼: {game_type}")
        self.log(f"   ğŸš© è§¦å‘å¼€å§‹(æœ€æ–°): {'ç›´æ¥å¼€å§‹' if not end_map else end_map}")
        self.log(f"   ğŸ›‘ è§¦å‘åœæ­¢(æœ€æ—§): {start_maps}")
        self.log("   âš¡ å·²å¯ç”¨å¤šçº¿ç¨‹åŠ é€Ÿ (Max: 10 threads)")

        page = 1
        collecting = True if not end_map else False
        collected_records = []
        combined_header = None
        stop_signal = False

        while self.is_running and not stop_signal:
            self.log(f"ğŸ“„ è¯·æ±‚ç¬¬ {page} é¡µæ•°æ®...")
            url = f"{self.base_url}/user/game?pageNum={page}&gameType={game_type_id}&mapCode="
            
            try:
                resp = self.session.get(url, timeout=10)
                soup = BeautifulSoup(resp.text, "html.parser")
            except Exception as e:
                self.log(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                break

            rows = soup.select("table.table tbody tr")
            if not rows:
                self.log("ğŸ“­ å·²æ— æ›´å¤šæ•°æ®")
                break

            # æ”¶é›†æœ¬é¡µéœ€è¦æŠ“å–çš„ä»»åŠ¡
            tasks = [] # (summary, url)

            for row in rows:
                if not self.is_running: break
                
                cols = [td.text.strip() for td in row.find_all("td")]
                if not cols or "æ²¡æœ‰å¯¹å±€" in cols[0]:
                    continue

                current_map = cols[1] if len(cols) > 1 else ""

                # 1. Start Condition
                if not collecting:
                    if current_map == end_map:
                        collecting = True
                        self.log(f"âœ… æ‰¾åˆ°ç»“æŸåœ°å›¾ [{end_map}]ï¼Œå¼€å§‹å½•åˆ¶...")
                    else:
                        continue

                # 2. Add to tasks
                if collecting:
                    summary = [cols[0], cols[1], cols[6]]
                    dt_tag = row.find("a", string="è¯¦æƒ…")
                    if dt_tag and dt_tag.has_attr("href"):
                        d_url = self.base_url + dt_tag["href"]
                        tasks.append((summary, d_url))

                    # 3. Stop Condition
                    if current_map in start_maps:
                        self.log(f"ğŸ æ‰¾åˆ°èµ·å§‹åœ°å›¾ [{current_map}]ï¼Œæœ¬é¡µå¤„ç†å®Œååœæ­¢ï¼")
                        stop_signal = True
                        # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½breakï¼Œå› ä¸ºæœ¬æ¡æ•°æ®ä¹Ÿéœ€è¦æŠ“å–
                        # ä½†éœ€è¦æ ‡è®°ä¸å†å¤„ç†åç»­çš„row
                        # ç®€å•å¤„ç†ï¼šæŠŠstop_signalè®¾ä¸ºTrueï¼Œå¾ªç¯ä¼šç»§ç»­æŠŠæœ¬é¡µå‰©ä¸‹çš„(å¦‚æœè¿˜åœ¨rowsé‡Œ)è¿‡ä¸€é? 
                        # ä¸ï¼Œåº”è¯¥æˆªæ–­ tasks åˆ—è¡¨å—ï¼Ÿ
                        # æ ¹æ®é€»è¾‘ï¼šé‡åˆ°èµ·å§‹åœ°å›¾ï¼Œè¿™ä¸€æ¡è¦æŠ“ï¼Œä½†æ›´æ—§çš„ä¸è¦äº†ã€‚
                        # æ‰€ä»¥æˆ‘ä»¬æ­¤æ—¶breakå‡º row å¾ªç¯ï¼Œä½†åœ¨breakå‰å·²ç»æŠŠå½“å‰ä»»åŠ¡åŠ è¿›å»äº†ã€‚
                        break
            
            # å¹¶å‘æ‰§è¡Œæœ¬é¡µçš„ä»»åŠ¡
            if tasks:
                self.log(f"   âš¡ æ­£åœ¨å¹¶å‘æŠ“å–æœ¬é¡µ {len(tasks)} åœºæ¯”èµ›è¯¦æƒ…...")
                with ThreadPoolExecutor(max_workers=20) as executor:
                    futures = [executor.submit(self.fetch_detail, t[0], t[1]) for t in tasks]
                    for future in as_completed(futures):
                        h, r = future.result()
                        if h and not combined_header:
                            combined_header = h
                        if r:
                            collected_records.extend(r)

            if stop_signal:
                break

            if soup.select_one("ul.pagination li.active + li a"):
                page += 1
                # å¤šçº¿ç¨‹æ¨¡å¼ä¸‹è¯·æ±‚éå¸¸å¿«ï¼Œè¿™é‡Œç¨å¾®å¤šç¡ä¸€ç‚¹ç‚¹é˜²æ­¢å°IPï¼Œæˆ–è€…ä¿æŒ0.5ä¹Ÿå¯
                time.sleep(0.5)
            else:
                self.log("âš ï¸ å·²ç¿»è‡³æœ€åä¸€é¡µï¼Œæœªæ‰¾åˆ°æŒ‡å®šçš„èµ·å§‹åœ°å›¾ï¼Œä½†å·²åœæ­¢ã€‚")
                break

        self.is_running = False
        self.log(f"ğŸ“Š å…±æŠ“å– {len(collected_records)} æ¡è®°å½•")
        
        final_data = self.clean_data(collected_records, substitution_map)

        if not combined_header and final_data:
            combined_header = ["æ¨¡å¼", "åœ°å›¾", "å¼€å§‹æ—¶é—´", "è§’è‰²", "è½¦è¾†", "é˜Ÿä¼", "æ’å", "æˆç»©", "ç»éªŒ", "é‡‘å¸"]

        return combined_header, final_data

    def stop(self):
        self.is_running = False
        self.log("ğŸ›‘ æ­£åœ¨åœæ­¢æŠ“å–...")

    def save_to_csv(self, header, records, output_dir):
        # ... (ä¿æŒä¸å˜)
        if not records:
            return None
            
        first = records[0]
        mode = first[0] if first else "æœªçŸ¥"
        date_str = first[2].split()[0] if len(first) > 2 else "00-00"
        filename = f"{date_str}_{mode}.csv"
        
        import os
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        out_path = os.path.join(output_dir, filename)

        try:
            with open(out_path, "w", encoding="utf-8-sig", newline="") as f:
                writer = csv.writer(f)
                if header:
                    writer.writerow(header)
                
                s_idx = header.index("æˆç»©") if header and "æˆç»©" in header else -1
                
                for row in records:
                    r = [str(x) for x in row]
                    if s_idx != -1 and s_idx < len(r) and ":" in r[s_idx]:
                        r[s_idx] = f"'{r[s_idx]}"
                    writer.writerow(r)
            
            self.log(f"âœ… CSVä¿å­˜æˆåŠŸ: {filename}")
            return out_path
        except Exception as e:
            self.log(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
            return None
